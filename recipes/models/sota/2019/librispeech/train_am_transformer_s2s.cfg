# Replace `[...]`, `[MODEL_DST]`, `[DATA_DST]`, with appropriate paths
--runname=am_transformer_seq2seq_librispeech
--rundir=[...]
--archdir=[...]
--arch=am_transformer_s2s.arch
--tokensdir=[MODEL_DST]/am
--tokens=librispeech-train-all-unigram-10000.tokens
--lexicon=[MODEL_DST]/am/librispeech-train+dev-unigram-10000-nbest10.lexicon
--train=[DATA_DST]/lists/train-clean-100.lst,[DATA_DST]/lists/train-clean-360.lst,[DATA_DST]/lists/train-other-500.lst
--valid=dev-clean:[DATA_DST]/lists/dev-clean.lst,dev-other:[DATA_DST]/lists/dev-other.lst
--criterion=transformer
--mfsc
--usewordpiece=true
--wordseparator=_
--am_decoder_tr_dropout=0.2
--am_decoder_tr_layerdrop=0.2
--am_decoder_tr_layers=6
--lmpct=0.0
--maxdecoderoutputlen=120
--labelsmooth=0.05
--dataorder=output_spiral
--inputbinsize=25
--attnWindow=softPretrain
--softwstd=4
--trainWithWindow=true
--pretrainWindow=3
--attention=keyvalue
--encoderdim=256
--memstepsize=5000000
--eostoken=true
--pcttraineval=1
--pctteacherforcing=99
--sampletarget=0.01
--netoptim=adagrad
--critoptim=adagrad
--lr=0.02
--lrcrit=0.02
--adambeta1=0.95
--adambeta2=0.98
--linseg=0
--momentum=0.95
--maxgradnorm=0.1
--onorm=target
--sqnorm
--nthread=6
--batchsize=2
--filterbanks=80
--speed=-1
--minloglevel=0
--logtostderr
--enable_distributed
