# Replace `[...]`, `[MODEL_DST]`, `[DATA_DST]`, with appropriate paths
--runname=am_transformer_seq2seq_librivox
--rundir=[...]
--archdir=[...]
--arch=am_arch/am_transformer_s2s_librivox.arch
--tokensdir=[MODEL_DST]/am
--tokens=librispeech-train-all-unigram-10000.tokens
--lexicon=[MODEL_DST]/am/librispeech-train-unigram-10000-nbest10.lexicon
--train=[DATA_DST]/lists/train-clean-100.lst,[DATA_DST]/lists/train-clean-360.lst,[DATA_DST]/lists/train-other-500.lst,[DATA_DST_librilight]/lists/librivox.lst
--valid=dev-clean:[DATA_DST]/lists/dev-clean.lst,dev-other:[DATA_DST]/lists/dev-other.lst
--target=ltr
--mfsc
--usewordpiece=true
--wordseparator=_
--criterion=transformer
--am_decoder_tr_dropout=0.2
--am_decoder_tr_layerdrop=0.2
--am_decoder_tr_layers=6
--maxdecoderoutputlen=120
--labelsmooth=0.05
--dataorder=output_spiral
--inputbinsize=25
--attnWindow=softPretrain
--softwstd=4
--trainWithWindow=true
--pretrainWindow=4000
--attention=keyvalue
--encoderdim=256
--memstepsize=5000000
--eostoken=true
--pcttraineval=1
--pctteacherforcing=99
--sampletarget=0.01
--netoptim=adagrad
--critoptim=adagrad
--lr=0.02
--lrcrit=0.02
--lr_decay=4
--lr_decay_step=3
--adambeta1=0.95
--adambeta2=0.99
--warmup=40001
--saug_start_update=40001
--linseg=0
--momentum=0.95
--maxgradnorm=1.0
--onorm=target
--sqnorm
--nthread=6
--batchsize=5
--filterbanks=80
--reportiters=3000
--enable_distributed
